{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Analyze Study (Binary Classification)\n",
    "- version 0.1\n",
    "- 3.2.2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ToDo\n",
    "- [done] Study details \n",
    "- [done] Target metric performance on all tasks \n",
    "- [done] Selected features summary\n",
    "- [done] Model performance on test dataset for a given task\n",
    "- [done] AUCROC plots\n",
    "- [done] Confusion matrix\n",
    "- Individual test feature importances (table + plot)\n",
    "- [done] Merged test feature importances (table + plot)\n",
    "- Summary confusion matrix\n",
    "- creat tests for notebook utils\n",
    "- beeswarm plot (individual + merged!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopus.predict import OctoPredict\n",
    "from octopus.predict.notebook_utils import (\n",
    "    show_selected_features,\n",
    "    show_study_details,\n",
    "    show_target_metric_performance,\n",
    "    testset_performance_overview,\n",
    "    plot_aucroc,\n",
    "    show_confusionmatrix,\n",
    "    show_overall_fi_table,\n",
    "    show_overall_fi_plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: Select study\n",
    "study_directory = \"../studies/wf_octo_mrmr_octo/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Study Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the utility function to display and validate study details\n",
    "study_info = show_study_details(study_directory)\n",
    "\n",
    "# Extract key variables for use in subsequent cells\n",
    "# path_study = study_info[\"path\"]\n",
    "# config = study_info[\"config\"]\n",
    "# ml_type = study_info[\"ml_type\"]\n",
    "# n_folds_outer = study_info[\"n_folds_outer\"]\n",
    "# workflow_tasks = study_info[\"workflow_tasks\"]\n",
    "# outersplit = study_info[\"outersplit_dirs\"]\n",
    "# expected_task_ids = study_info[\"expected_task_ids\"]\n",
    "# octo_workflow_lst = study_info[\"octo_workflow_tasks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Target Metric Performance for all  Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance (target metric) for all workflow tasks\n",
    "performance_tables = show_target_metric_performance(study_info, details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Selected Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of selected features across outer splits and tasks\n",
    "# Returns three tables: feature count table, feature frequency table, and raw performance dataframe\n",
    "# sort_task and sort_key parameters sort the frequency table by the specified task-key combination\n",
    "sort_by_task = None\n",
    "sort_by_key = None\n",
    "feature_table, feature_frequency_table, raw_feature_table = show_selected_features(study_info, sort_task=sort_by_task, sort_key=sort_by_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Model Performance on Test Dataset for a given Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: selected metrics for performance overview\n",
    "metrics = [\"AUCROC\", \"ACCBAL\", \"ACC\", \"F1\", \"AUCPR\", \"NEGBRIERSCORE\"]\n",
    "print(\"Selected metrics: \", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Test performance for given task and selected metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load predictor object\n",
    "task_predictor = OctoPredict(study_path=study_info[\"path\"], task_id=0, results_key=\"best\")\n",
    "testset_performance = testset_performance_overview(predictor=task_predictor, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### AUCROC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aucroc(task_predictor, show_individual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusionmatrix(task_predictor, threshold=0.5, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Test Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Calculate Permutation Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Permutation feature importances on test data using final models\n",
    "# - fi tables are saved in the  study.results dictionary\n",
    "# - pdf plots are saved in the results directory of the sequence item\n",
    "#\n",
    "# calculate pfi for only one experiment\n",
    "# task_predictor.calculate_fi_test(fi_type=\"group_permutation\", n_repeat=5, experiment_id=4)\n",
    "#\n",
    "# calculate pfi for all available experiments\n",
    "# - n_repeats has major impact on p-values\n",
    "# - high n_repeats lead to long compute times\n",
    "print(\"PFI calculation running.....\")\n",
    "task_predictor.calculate_fi_test(fi_type=\"group_permutation\", n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_table_overall = show_overall_fi_table(task_predictor, fi_type=\"group_permutation\")\n",
    "fi_table_overall.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_overall_fi_plot(task_predictor, fi_type=\"group_permutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Calculate Shap Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (D) Shap feature importances on test data using final models\n",
    "# \n",
    "# - for highest quality use \"exact\" or \"kernel\"\n",
    "# - shap_type could be [\"kernel\", \"permutation\", \"exact\"]\n",
    "# - shap_type \"exact\" does not scale well with number of features\n",
    "# - shap_type \"permutation\" scales better than \"exact\" but\n",
    "#   takes longer for a small number of features\n",
    "# - shap_type \"kernel\" does scales better than \"exact\" but is slower than \"permutation\"\n",
    "# - fi tables are saved in the  study.results dictionary\n",
    "# - pdf plots are saved in the results directory\n",
    "task_predictor.calculate_fi_test(fi_type=\"shap\", shap_type=\"kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_table_overall = show_overall_fi_table(task_predictor, fi_type=\"shap\")\n",
    "fi_table_overall.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_overall_fi_plot(task_predictor, fi_type=\"shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
