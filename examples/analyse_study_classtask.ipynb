{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb14914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze Study (Classification Task) - Jupyter notebook script.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Analyze Study (Classification Task) - Jupyter notebook script.\"\"\"\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.18.1\n",
    "#   kernelspec:\n",
    "#     display_name: octopus\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee73288",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fd90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopus.predict import OctoPredict\n",
    "from octopus.predict.notebook_utils import (\n",
    "    show_selected_features,\n",
    "    show_study_details,\n",
    "    show_target_metric_performance,\n",
    "    testset_performance_overview,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89dbee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Analyze Study (Classification Task)\n",
    "- version 0.1\n",
    "- 2025.01.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642115e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ToDo\n",
    "\n",
    "- create predict directory\n",
    "- create utility functions in separate file\n",
    "- functionality:\n",
    "  1. study overview: which workflow tasks, number of splits\n",
    "  2. performance overview for certain given metric\n",
    "  3. provide feature lists for each task\n",
    "- aucpr -- baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de56e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: Select study\n",
    "study_directory = \"./studies/workflow_sequential_tasks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411a076",
   "metadata": {},
   "source": [
    "## Show Study Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9935e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected study path: studies/workflow_sequential_tasks\n",
      "\n",
      "Validate study....\n",
      "ML Type: classification\n",
      "Found 5 outersplit directory/directories\n",
      "Expected outersplit IDs: [0, 1, 2, 3, 4]\n",
      "All expected outersplit directories found\n",
      "Expected workflow task IDs: [0, 1, 2]\n",
      "Study has completed workflow tasks - all expected directories found\n",
      "\n",
      "Information on workflow tasks in this study\n",
      "Number of workflow tasks: 3\n",
      "Task 0: octo\n",
      "Task 1: mrmr\n",
      "Task 2: octo\n",
      "Octo workflow tasks: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Call the utility function to display and validate study details\n",
    "study_info = show_study_details(study_directory, expected_ml_type=\"classification\")\n",
    "\n",
    "# Extract key variables for use in subsequent cells\n",
    "# path_study = study_info[\"path\"]\n",
    "# config = study_info[\"config\"]\n",
    "# ml_type = study_info[\"ml_type\"]\n",
    "# n_folds_outer = study_info[\"n_folds_outer\"]\n",
    "# workflow_tasks = study_info[\"workflow_tasks\"]\n",
    "# outersplit = study_info[\"outersplit_dirs\"]\n",
    "# expected_task_ids = study_info[\"expected_task_ids\"]\n",
    "# octo_workflow_lst = study_info[\"octo_workflow_tasks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e0dea",
   "metadata": {},
   "source": [
    "## Show Target Metric Performance for all  Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWorkflow task: 0\u001b[0m\n",
      "Available results keys: ['best']\n",
      "Selected results key: best\n",
      "            train_avg   dev_avg  test_avg  train_pool  dev_pool  test_pool\n",
      "OuterSplit                                                                \n",
      "0            1.000000  0.695751  0.654031    1.000000  0.696364   0.640554\n",
      "1            0.988055  0.702480  0.670525    0.987525  0.701758   0.670318\n",
      "2            0.919484  0.661212  0.663539    0.939724  0.660001   0.673005\n",
      "3            1.000000  0.712161  0.652500    1.000000  0.710867   0.658333\n",
      "4            0.996599  0.688055  0.670833    0.996914  0.684593   0.675000\n",
      "Mean         0.980828  0.691932  0.662286    0.984832  0.690717   0.663442\n",
      "\u001b[1mWorkflow task: 1\u001b[0m\n",
      "Available results keys: []\n",
      "\u001b[1mWorkflow task: 2\u001b[0m\n",
      "Available results keys: ['best']\n",
      "Selected results key: best\n",
      "            train_avg   dev_avg  test_avg  train_pool  dev_pool  test_pool\n",
      "OuterSplit                                                                \n",
      "0            1.000000  0.714446  0.650682    1.000000  0.714493   0.617817\n",
      "1            0.920360  0.715059  0.670442    0.929211  0.716931   0.731294\n",
      "2            0.990847  0.675627  0.725424    0.995798  0.666822   0.754031\n",
      "3            0.972664  0.754196  0.680000    0.983511  0.750356   0.687500\n",
      "4            0.862951  0.711126  0.595833    0.871809  0.711423   0.612500\n",
      "Mean         0.949364  0.714091  0.664476    0.956066  0.712005   0.680628\n"
     ]
    }
   ],
   "source": [
    "# Display performance (target metric) for all workflow tasks\n",
    "df_performance = show_target_metric_performance(study_info, details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1760f13",
   "metadata": {},
   "source": [
    "## Show Selected Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NUMBER OF SELECTED FEATURES\n",
      "================================================================================\n",
      "Rows: OuterSplit | Columns: Task ID\n",
      "Task           0     2\n",
      "OuterSplit            \n",
      "0           30.0  15.0\n",
      "1           30.0  15.0\n",
      "2           30.0  15.0\n",
      "3           30.0  15.0\n",
      "4           30.0  15.0\n",
      "Mean        30.0  15.0\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE FREQUENCY ACROSS OUTER SPLITS\n",
      "================================================================================\n",
      "Rows: Features | Columns: Task ID\n",
      "Sorted by Task 0 frequency (highest first)\n",
      "            0  2\n",
      "feature_00  5  2\n",
      "feature_01  5  3\n",
      "feature_02  5  5\n",
      "feature_03  5  0\n",
      "feature_04  5  1\n",
      "feature_05  5  0\n",
      "feature_06  5  1\n",
      "feature_07  5  0\n",
      "feature_08  5  4\n",
      "feature_09  5  1\n",
      "feature_10  5  1\n",
      "feature_11  5  3\n",
      "feature_12  5  3\n",
      "feature_13  5  5\n",
      "feature_14  5  4\n",
      "feature_15  5  3\n",
      "feature_16  5  2\n",
      "feature_17  5  2\n",
      "feature_18  5  1\n",
      "feature_19  5  4\n",
      "feature_20  5  1\n",
      "feature_21  5  2\n",
      "feature_22  5  2\n",
      "feature_23  5  4\n",
      "feature_24  5  2\n",
      "feature_25  5  2\n",
      "feature_26  5  3\n",
      "feature_27  5  4\n",
      "feature_28  5  5\n",
      "feature_29  5  5\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the number of selected features across outer splits and tasks\n",
    "# Returns two tables: feature counts and feature frequency\n",
    "# sort_task parameter sorts the frequency table by the specified task\n",
    "sort_by_task = None\n",
    "feature_numbers_table, feature_frequency_table = show_selected_features(df_performance, sort_task=sort_by_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ccf59",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance on Test Dataset for a given Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd5cd04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected task: 0\n",
      "Selected results_key: best\n",
      "\n",
      "\n",
      "Loading available experiments ......\n",
      "Outersplit0, task0 found.\n",
      "Outersplit1, task0 found.\n",
      "Outersplit2, task0 found.\n",
      "Outersplit3, task0 found.\n",
      "Outersplit4, task0 found.\n",
      "5 experiment(s) out of 5 found.\n"
     ]
    }
   ],
   "source": [
    "# Input: select task\n",
    "selected_task = 0\n",
    "\n",
    "# check available results_keys in cell output below\n",
    "results_key = \"best\"\n",
    "\n",
    "print(\"Selected task:\", selected_task)\n",
    "print(\"Selected results_key:\", results_key)\n",
    "print()\n",
    "# load predictor object\n",
    "task_predictor = OctoPredict(study_path=study_info[\"path\"], task_id=selected_task, results_key=results_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad836282",
   "metadata": {},
   "source": [
    "### Testset Performance overview for Selected Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b600c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected metrics:  ['AUCROC', 'ACCBAL', 'ACC', 'F1', 'AUCPR', 'NEGBRIERSCORE']\n"
     ]
    }
   ],
   "source": [
    "# Input: selected metrics for performance overviwe\n",
    "metrics = [\"AUCROC\", \"ACCBAL\", \"ACC\", \"F1\", \"AUCPR\", \"NEGBRIERSCORE\"]\n",
    "print(\"Selected metrics: \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3fef182",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset (pooling)\n",
      "              AUCROC    ACCBAL    ACC        F1     AUCPR  NEGBRIERSCORE\n",
      "outersplit                                                              \n",
      "0           0.779661  0.640554  0.690  0.491803  0.739346       0.203061\n",
      "1           0.750310  0.670318  0.690  0.597403  0.699329       0.207333\n",
      "2           0.756098  0.673005  0.680  0.619048  0.671229       0.219743\n",
      "3           0.749583  0.658333  0.700  0.545455  0.704472       0.203279\n",
      "4           0.756250  0.675000  0.710  0.579710  0.705946       0.206924\n",
      "Mean        0.758380  0.663442  0.694  0.566684  0.704064       0.208068\n"
     ]
    }
   ],
   "source": [
    "testset_performance = testset_performance_overview(predictor=task_predictor, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "octopus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
