{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb14914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze Study (Classification Task) - Jupyter notebook script.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Analyze Study (Classification Task) - Jupyter notebook script.\"\"\"\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.18.1\n",
    "#   kernelspec:\n",
    "#     display_name: octopus\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee73288",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0fd90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopus.predict import OctoPredict\n",
    "from octopus.predict.notebook_utils import (\n",
    "    show_selected_features,\n",
    "    show_study_details,\n",
    "    show_target_metric_performance,\n",
    "    testset_performance_overview,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89dbee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Analyze Study (Classification Task)\n",
    "- version 0.1\n",
    "- 2025.01.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642115e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ToDo\n",
    "\n",
    "- create predict directory\n",
    "- create utility functions in separate file\n",
    "- functionality:\n",
    "  1. study overview: which workflow tasks, number of splits\n",
    "  2. performance overview for certain given metric\n",
    "  3. provide feature lists for each task\n",
    "- aucpr -- baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de56e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: Select study\n",
    "study_directory = \"./studies/example_octo_autogluon_parallel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411a076",
   "metadata": {},
   "source": [
    "## Show Study Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9935e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected study path: studies/example_octo_autogluon_parallel\n",
      "\n",
      "Validate study....\n",
      "ML Type: classification\n",
      "Found 5 outersplit directory/directories\n",
      "Expected outersplit IDs: [0, 1, 2, 3, 4]\n",
      "All expected outersplit directories found\n",
      "Expected workflow task IDs: [0, 1]\n",
      "Study has completed workflow tasks - all expected directories found\n",
      "\n",
      "Information on workflow tasks in this study\n",
      "Number of workflow tasks: 2\n",
      "Task 0: octo\n",
      "Task 1: autogluon\n",
      "Octo workflow tasks: [0]\n"
     ]
    }
   ],
   "source": [
    "# Call the utility function to display and validate study details\n",
    "study_info = show_study_details(study_directory, expected_ml_type=\"classification\")\n",
    "\n",
    "# Extract key variables for use in subsequent cells\n",
    "# path_study = study_info[\"path\"]\n",
    "# config = study_info[\"config\"]\n",
    "# ml_type = study_info[\"ml_type\"]\n",
    "# n_folds_outer = study_info[\"n_folds_outer\"]\n",
    "# workflow_tasks = study_info[\"workflow_tasks\"]\n",
    "# outersplit = study_info[\"outersplit_dirs\"]\n",
    "# expected_task_ids = study_info[\"expected_task_ids\"]\n",
    "# octo_workflow_lst = study_info[\"octo_workflow_tasks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e0dea",
   "metadata": {},
   "source": [
    "## Show Target Metric Performance for all  Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "006432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWorkflow task: 0\u001b[0m\n",
      "Available results keys: ['best']\n",
      "Selected results key: best\n",
      "            train_avg   dev_avg  test_avg  train_pool  dev_pool  test_pool\n",
      "OuterSplit                                                                \n",
      "0            0.901713  0.734636  0.790278    0.917433  0.732466   0.810185\n",
      "1            0.896212  0.778854  0.788657    0.908163  0.769546   0.807870\n",
      "2            0.903516  0.752681  0.743981    0.921314  0.747126   0.765046\n",
      "3            0.914822  0.811106  0.660571    0.922212  0.796482   0.673143\n",
      "4            0.938804  0.759212  0.851657    0.963449  0.733256   0.872000\n",
      "Mean         0.911014  0.767298  0.767029    0.926514  0.755775   0.785649\n",
      "\u001b[1mWorkflow task: 1\u001b[0m\n",
      "Available results keys: ['autogluon']\n",
      "Selected results key: autogluon\n",
      "            score_val_dev  pred_time_val_dev  pred_time_val_marginal_dev  \\\n",
      "OuterSplit                                                                 \n",
      "0                0.780720           0.114040                    0.114040   \n",
      "1                0.801775           0.116555                    0.116555   \n",
      "2                0.815320           0.116296                    0.116296   \n",
      "3                0.836385           0.161272                    0.161272   \n",
      "4                0.769087           0.116342                    0.116342   \n",
      "Mean             0.800657           0.124901                    0.124901   \n",
      "\n",
      "            roc_auc_train  accuracy_train  balanced_accuracy_train  mcc_train  \\\n",
      "OuterSplit                                                                      \n",
      "0                     1.0             1.0                      1.0        1.0   \n",
      "1                     1.0             1.0                      1.0        1.0   \n",
      "2                     1.0             1.0                      1.0        1.0   \n",
      "3                     1.0             1.0                      1.0        1.0   \n",
      "4                     1.0             1.0                      1.0        1.0   \n",
      "Mean                  1.0             1.0                      1.0        1.0   \n",
      "\n",
      "            f1_train  precision_train  recall_train  ...  \\\n",
      "OuterSplit                                           ...   \n",
      "0                1.0              1.0           1.0  ...   \n",
      "1                1.0              1.0           1.0  ...   \n",
      "2                1.0              1.0           1.0  ...   \n",
      "3                1.0              1.0           1.0  ...   \n",
      "4                1.0              1.0           1.0  ...   \n",
      "Mean             1.0              1.0           1.0  ...   \n",
      "\n",
      "            balanced_accuracy_test  mcc_test   f1_test  precision_test  \\\n",
      "OuterSplit                                                               \n",
      "0                         0.694444  0.492366  0.571429        0.909091   \n",
      "1                         0.743056  0.549972  0.666667        0.866667   \n",
      "2                         0.743056  0.549972  0.666667        0.866667   \n",
      "3                         0.585714  0.281718  0.322581        0.833333   \n",
      "4                         0.785714  0.637059  0.731707        0.937500   \n",
      "Mean                      0.710397  0.502217  0.591810        0.882652   \n",
      "\n",
      "            recall_test  AUCROC_test_octo  ACCBAL_test_octo  ACC_test_octo  \\\n",
      "OuterSplit                                                                   \n",
      "0              0.416667          0.892940          0.715278       0.766667   \n",
      "1              0.541667          0.866319          0.743056       0.783333   \n",
      "2              0.541667          0.851273          0.743056       0.783333   \n",
      "3              0.200000          0.774857          0.585714       0.650000   \n",
      "4              0.600000          0.908000          0.805714       0.833333   \n",
      "Mean           0.460000          0.858678          0.718563       0.763333   \n",
      "\n",
      "            LOGLOSS_test_octo  F1_test_octo  \n",
      "OuterSplit                                   \n",
      "0                    0.545923      0.611111  \n",
      "1                    0.584026      0.666667  \n",
      "2                    0.545961      0.666667  \n",
      "3                    0.602722      0.322581  \n",
      "4                    0.541459      0.761905  \n",
      "Mean                 0.564018      0.605786  \n",
      "\n",
      "[6 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display performance (target metric) for all workflow tasks\n",
    "df_performance = show_target_metric_performance(study_info, details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1760f13",
   "metadata": {},
   "source": [
    "## Show Selected Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e2e58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NUMBER OF SELECTED FEATURES\n",
      "================================================================================\n",
      "Rows: OuterSplit | Columns: Task ID\n",
      "Task            0       1\n",
      "OuterSplit               \n",
      "0           169.0  1000.0\n",
      "1           131.0  1000.0\n",
      "2           173.0  1000.0\n",
      "3           160.0  1000.0\n",
      "4           174.0  1000.0\n",
      "Mean        161.4  1000.0\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE FREQUENCY ACROSS OUTER SPLITS\n",
      "================================================================================\n",
      "Rows: Features | Columns: Task ID\n",
      "Sorted by Task 0 frequency (highest first)\n",
      "                0  1\n",
      "informative_22  5  5\n",
      "informative_2   5  5\n",
      "informative_11  5  5\n",
      "informative_23  5  5\n",
      "noise_124       5  5\n",
      "...            .. ..\n",
      "redundant_83    0  5\n",
      "redundant_84    0  5\n",
      "redundant_85    0  5\n",
      "redundant_86    0  5\n",
      "informative_0   0  5\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the number of selected features across outer splits and tasks\n",
    "# Returns two tables: feature counts and feature frequency\n",
    "# sort_task parameter sorts the frequency table by the specified task\n",
    "sort_by_task = None\n",
    "feature_numbers_table, feature_frequency_table = show_selected_features(df_performance, sort_task=sort_by_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ccf59",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance on Test Dataset for a given Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fd5cd04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading available experiments ......\n",
      "Outersplit0, task0 found.\n",
      "Outersplit1, task0 found.\n",
      "Outersplit2, task0 found.\n",
      "Outersplit3, task0 found.\n",
      "Outersplit4, task0 found.\n",
      "5 experiment(s) out of 5 found.\n",
      "\n",
      "Loading available experiments ......\n",
      "Outersplit0, task1 found.\n",
      "Outersplit1, task1 found.\n",
      "Outersplit2, task1 found.\n",
      "Outersplit3, task1 found.\n",
      "Outersplit4, task1 found.\n",
      "5 experiment(s) out of 5 found.\n"
     ]
    }
   ],
   "source": [
    "# load predictor object\n",
    "task_predictor_octo = OctoPredict(study_path=study_info[\"path\"], task_id=0, results_key=\"best\")\n",
    "task_predictor_ag = OctoPredict(study_path=study_info[\"path\"], task_id=1, results_key=\"autogluon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad836282",
   "metadata": {},
   "source": [
    "### Testset Performance overview for Selected Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b600c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected metrics:  ['AUCROC', 'ACCBAL', 'ACC', 'F1', 'AUCPR', 'NEGBRIERSCORE']\n"
     ]
    }
   ],
   "source": [
    "# Input: selected metrics for performance overviwe\n",
    "metrics = [\"AUCROC\", \"ACCBAL\", \"ACC\", \"F1\", \"AUCPR\", \"NEGBRIERSCORE\"]\n",
    "print(\"Selected metrics: \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fef182",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset (pooling)\n",
      "              AUCROC    ACCBAL       ACC        F1     AUCPR  NEGBRIERSCORE\n",
      "outersplit                                                                 \n",
      "0           0.810185  0.729167  0.733333  0.680000  0.772617       0.218037\n",
      "1           0.807870  0.763889  0.750000  0.727273  0.682217       0.221833\n",
      "2           0.765046  0.680556  0.700000  0.608696  0.758529       0.210986\n",
      "3           0.673143  0.551429  0.583333  0.418605  0.591645       0.231660\n",
      "4           0.872000  0.757143  0.750000  0.727273  0.828682       0.201398\n",
      "Mean        0.785649  0.696437  0.703333  0.632369  0.726738       0.216783\n"
     ]
    }
   ],
   "source": [
    "testset_performance_octo = testset_performance_overview(predictor=task_predictor_octo, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6a5cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset (pooling)\n",
      "              AUCROC    ACCBAL       ACC        F1     AUCPR  NEGBRIERSCORE\n",
      "outersplit                                                                 \n",
      "0           0.892940  0.715278  0.766667  0.611111  0.849247       0.180301\n",
      "1           0.866319  0.743056  0.783333  0.666667  0.788766       0.197309\n",
      "2           0.851273  0.743056  0.783333  0.666667  0.825113       0.180651\n",
      "3           0.774857  0.585714  0.650000  0.322581  0.725365       0.207583\n",
      "4           0.908000  0.805714  0.833333  0.761905  0.902734       0.177163\n",
      "Mean        0.858678  0.718563  0.763333  0.605786  0.818245       0.188601\n"
     ]
    }
   ],
   "source": [
    "testset_performance_ag = testset_performance_overview(predictor=task_predictor_ag, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c6685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "octopus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
