{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb14914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze Study (Classification Task) - Jupyter notebook script.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Analyze Study (Classification Task) - Jupyter notebook script.\"\"\"\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.18.1\n",
    "#   kernelspec:\n",
    "#     display_name: octopus\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee73288",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0fd90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopus.predict import OctoPredict\n",
    "from octopus.predict.notebook_utils import (\n",
    "    show_selected_features,\n",
    "    show_study_details,\n",
    "    show_target_metric_performance,\n",
    "    testset_performance_overview,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89dbee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Analyze Study (Classification Task)\n",
    "- version 0.1\n",
    "- 2025.01.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642115e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ToDo\n",
    "\n",
    "- create predict directory\n",
    "- create utility functions in separate file\n",
    "- functionality:\n",
    "  1. study overview: which workflow tasks, number of splits\n",
    "  2. performance overview for certain given metric\n",
    "  3. provide feature lists for each task\n",
    "- aucpr -- baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de56e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d9690c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: Select study\n",
    "study_directory = \"../studies/example_octo_autogluon_parallel_split0/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411a076",
   "metadata": {},
   "source": [
    "## Show Study Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9935e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected study path: ../studies/example_octo_autogluon_parallel_split0\n",
      "\n",
      "Validate study....\n",
      "ML Type: classification\n",
      "Found 1 outersplit directory/directories\n",
      "Expected outersplit IDs: [0, 1, 2, 3, 4]\n",
      "⚠️  WARNING: Missing directory 'outersplit1'\n",
      "⚠️  WARNING: Missing directory 'outersplit2'\n",
      "⚠️  WARNING: Missing directory 'outersplit3'\n",
      "⚠️  WARNING: Missing directory 'outersplit4'\n",
      "⚠️  4 outersplit directory/directories missing\n",
      "Expected workflow task IDs: [0, 1]\n",
      "Study has completed workflow tasks - all expected directories found\n",
      "\n",
      "Information on workflow tasks in this study\n",
      "Number of workflow tasks: 2\n",
      "Task 0: octo\n",
      "Task 1: autogluon\n",
      "Octo workflow tasks: [0]\n"
     ]
    }
   ],
   "source": [
    "# Call the utility function to display and validate study details\n",
    "study_info = show_study_details(study_directory, expected_ml_type=\"classification\")\n",
    "\n",
    "# Extract key variables for use in subsequent cells\n",
    "# path_study = study_info[\"path\"]\n",
    "# config = study_info[\"config\"]\n",
    "# ml_type = study_info[\"ml_type\"]\n",
    "# n_folds_outer = study_info[\"n_folds_outer\"]\n",
    "# workflow_tasks = study_info[\"workflow_tasks\"]\n",
    "# outersplit = study_info[\"outersplit_dirs\"]\n",
    "# expected_task_ids = study_info[\"expected_task_ids\"]\n",
    "# octo_workflow_lst = study_info[\"octo_workflow_tasks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e0dea",
   "metadata": {},
   "source": [
    "## Show Target Metric Performance for all  Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "006432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWorkflow task: 0\u001b[0m\n",
      "Available results keys: ['best']\n",
      "Selected results key: best\n",
      "            train_avg   dev_avg  test_avg  train_pool  dev_pool  test_pool\n",
      "OuterSplit                                                                \n",
      "0            0.962172  0.835633  0.744667    0.975069  0.828056   0.771111\n",
      "Mean         0.962172  0.835633  0.744667    0.975069  0.828056   0.771111\n",
      "\u001b[1mWorkflow task: 1\u001b[0m\n",
      "Available results keys: ['autogluon']\n",
      "Selected results key: autogluon\n",
      "            score_val_dev  pred_time_val_dev  pred_time_val_marginal_dev  \\\n",
      "OuterSplit                                                                 \n",
      "0                0.737396           0.112737                    0.112737   \n",
      "Mean             0.737396           0.112737                    0.112737   \n",
      "\n",
      "            roc_auc_train  accuracy_train  balanced_accuracy_train  mcc_train  \\\n",
      "OuterSplit                                                                      \n",
      "0                     1.0             1.0                      1.0        1.0   \n",
      "Mean                  1.0             1.0                      1.0        1.0   \n",
      "\n",
      "            f1_train  precision_train  recall_train  ...  \\\n",
      "OuterSplit                                           ...   \n",
      "0                1.0              1.0           1.0  ...   \n",
      "Mean             1.0              1.0           1.0  ...   \n",
      "\n",
      "            balanced_accuracy_test  mcc_test   f1_test  precision_test  \\\n",
      "OuterSplit                                                               \n",
      "0                         0.566667  0.133333  0.566667        0.566667   \n",
      "Mean                      0.566667  0.133333  0.566667        0.566667   \n",
      "\n",
      "            recall_test  AUCROC_test_octo  ACCBAL_test_octo  ACC_test_octo  \\\n",
      "OuterSplit                                                                   \n",
      "0              0.566667          0.598889          0.533333       0.533333   \n",
      "Mean           0.566667          0.598889          0.533333       0.533333   \n",
      "\n",
      "            LOGLOSS_test_octo  F1_test_octo  \n",
      "OuterSplit                                   \n",
      "0                    0.678604      0.548387  \n",
      "Mean                 0.678604      0.548387  \n",
      "\n",
      "[2 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display performance (target metric) for all workflow tasks\n",
    "df_performance = show_target_metric_performance(study_info, details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1760f13",
   "metadata": {},
   "source": [
    "## Show Selected Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e2e58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NUMBER OF SELECTED FEATURES\n",
      "================================================================================\n",
      "Rows: OuterSplit | Columns: Task ID\n",
      "Task            0       1\n",
      "OuterSplit               \n",
      "0           237.0  1000.0\n",
      "Mean        237.0  1000.0\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FEATURE FREQUENCY ACROSS OUTER SPLITS\n",
      "================================================================================\n",
      "Rows: Features | Columns: Task ID\n",
      "Sorted by Task 0 frequency (highest first)\n",
      "                0  1\n",
      "informative_0   1  1\n",
      "informative_1   1  1\n",
      "informative_10  1  1\n",
      "informative_11  1  1\n",
      "informative_16  1  1\n",
      "...            .. ..\n",
      "redundant_29    0  1\n",
      "redundant_4     0  1\n",
      "redundant_5     0  1\n",
      "redundant_6     0  1\n",
      "redundant_8     0  1\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the number of selected features across outer splits and tasks\n",
    "# Returns two tables: feature counts and feature frequency\n",
    "# sort_task parameter sorts the frequency table by the specified task\n",
    "sort_by_task = None\n",
    "feature_numbers_table, feature_frequency_table = show_selected_features(df_performance, sort_task=sort_by_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ccf59",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance on Test Dataset for a given Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fd5cd04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading available experiments ......\n",
      "Outersplit0, task0 found.\n",
      "1 experiment(s) out of 5 found.\n",
      "\n",
      "Loading available experiments ......\n",
      "Outersplit0, task1 found.\n",
      "1 experiment(s) out of 5 found.\n"
     ]
    }
   ],
   "source": [
    "# load predictor object\n",
    "task_predictor_octo = OctoPredict(study_path=study_info[\"path\"], task_id=0, results_key=\"best\")\n",
    "task_predictor_ag = OctoPredict(study_path=study_info[\"path\"], task_id=1, results_key=\"autogluon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad836282",
   "metadata": {},
   "source": [
    "### Testset Performance overview for Selected Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4b600c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected metrics:  ['AUCROC', 'ACCBAL', 'ACC', 'F1', 'AUCPR', 'NEGBRIERSCORE']\n"
     ]
    }
   ],
   "source": [
    "# Input: selected metrics for performance overviwe\n",
    "metrics = [\"AUCROC\", \"ACCBAL\", \"ACC\", \"F1\", \"AUCPR\", \"NEGBRIERSCORE\"]\n",
    "print(\"Selected metrics: \", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e3fef182",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset (pooling)\n",
      "              AUCROC    ACCBAL       ACC       F1    AUCPR  NEGBRIERSCORE\n",
      "outersplit                                                               \n",
      "0           0.771111  0.633333  0.633333  0.65625  0.79296       0.231665\n",
      "Mean        0.771111  0.633333  0.633333  0.65625  0.79296       0.231665\n"
     ]
    }
   ],
   "source": [
    "testset_performance_octo = testset_performance_overview(predictor=task_predictor_octo, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6a5cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset (pooling)\n",
      "              AUCROC    ACCBAL       ACC        F1     AUCPR  NEGBRIERSCORE\n",
      "outersplit                                                                 \n",
      "0           0.598889  0.533333  0.533333  0.548387  0.641905       0.242798\n",
      "Mean        0.598889  0.533333  0.533333  0.548387  0.641905       0.242798\n"
     ]
    }
   ],
   "source": [
    "testset_performance_ag = testset_performance_overview(predictor=task_predictor_ag, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c6685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "octopus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
